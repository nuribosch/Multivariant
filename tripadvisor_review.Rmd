---
title: "Promoció d'activitats d'oci per part d'ajuntaments."
author: "Núria Bosch, Toni Bosch, Heribert Roig"
date: "29/5/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducció
Recentment, la Unió Europea ha destinat un seguit de fons econòmics per a la cooperació i el desenvolupament social dels pobles i ciutats de menys de 30.000 habitants. 
Aquests fons han d'anar destinats a la promoció d'activitats d'oci com ara: Galeries d'Art, Espais de dansa, Bars, Restaurants, Museus, Hotels/Resorts, Espais de "Picnic", Platges, Teatres i Institucions Religioses.

Per a l'obtenció del fons, cada una de les ciutats que hi apliquin han de complir amb els següents requisits:

a) Els fons s'han d'invertir a través de promocions de les diferents activitats d'oci.

b) Les promocions han d'anar dirigides a un públic interessat en l'activitat d'oci en qüestió per augmentar l'eficiència de l'ajut.

c) Les campanyes de promoció de les activitats, ha d'intentar anar focalitzades a conjunts de persones(interessos similars per les activitats d'oci promocionades) amb la finalitat de rendibilitzar les inversions dels ajuntaments.

Per tant, el que es vol desenvolupar és aconseguir categoritzar el interessos de la població per a invertir de la millor forma els ajuts proporcionats per la Unió Europea.

## Dataset
Explicació del dataset i les variables

La base de dades l'hem extret de la pàgina web de la Universitat de Califòrnia a partir del següent enllaç: https://archive.ics.uci.edu/ml/datasets/Travel+Reviews#

```{r}
data<-read.csv("tripadvisor_review.csv")
colnames(data) <- c("User_ID", "Art_gallery", "Dance_clubs", "Juice_bars", "Restaurants", "Museums", "Resorts", "Parks_picnic_spots", "Beaches", "Theaters", "Religious_inst.")
str(data)
head(data)
```

## Objectius

A partir de la descripció del problema, els objectius que tindrem en aquest treball seran els següents:

* Determinar quines són les activitats amb un interès major.

* Determinar si podem agrupar els usuaris segons l'interès en les activitats.

* Determinar si l'interès està correlacionat en diferents activitats (Si el mateix grup de persones està interessat principalment en dos activitats, no les podem programar alhora).

## Abast

* Considerarem que només podem programar X activitats.

* Considerarem que els usuaris només participaran en Y activitats durant la seva visita.

* ...

# Anàlisi descriptiu

Com que la variable *ID* no ens aporta valor, ja que cada observació correspon a un usuari diferent, decidim eliminar-la

Avaluem el número de missing data que hi ha al conjunt de dades:
```{r}
# length(unique(data$User_ID))
data$User_ID <- NULL
sum(is.na(data))
```

```{r}
summary(data)
```

Veiem que totes les variables estan dintre del rang (0,4). Considerem que no fa falta escalar les dades, ja que totes les variables estan en les mateixes unitats.

Mostrem un boxplot per cadascuna de les variables

```{r}
par(mfrow=c(2,2))

for (i in (1:(dim(data)[2]))){
  boxplot(data[,i], ylim = c(0, 4))
  title(colnames(data)[i])
}

par(mfrow=c(1,1))
```

Veiem que hi ha algunes variables que tenen major variabilitat que d'altres. Per exemple la puntuació mitjana en bars de sucs té una variabilitat molt més alta que la puntuació mitjana en parcs i llocs de pícnic.

Correlació de variables 2 a 2

```{r}
library(corrplot)
C <- cor(data)
corrplot(C, method = 'number')
```

Veiem que les variables més correlacionades són *parks_picnic_spots* amb *juice_bars* i *Religious_inst*.

# Distàncies entre individus
Els valors de les variables són valors continus i considerem que estan en les mateixes unitats. Com que veiem que hi ha certa correlació entre variables, utilitzarem la distància de Mahalanobis. El 0, en aquesta base de dades no té un valor qualitatiu, sinó quantitatiu. En cas que tingués un significat qualitatiu, hauríem d'utilitzar una altra distància per perfils. (SI NO ELS ESCALEM, HI HAURÀ VARIABLES QUE TINDRAN CONTRIBUCIONS BAIXES EN ELS MÈTODES DE COMPONENTS PRINCIPALS)

Càlcul de la distància:
```{r}
library(StatMatch)
X<-data
# X<-scale(X) #scale
D<-dist(X)  #euclidean distance
DM<-as.dist(mahalanobis.dist(data)) #mahalanobis distance

```

# Clustering

## Clustering jeràrquic
```{r}
# Euclidean
clusterAv.E<-hclust(D,method= "average")
clusterW.E<-hclust(D,method= "ward.D2")
clusterS.E<-hclust(D,method= "single")
clusterC.E<-hclust(D,method= "complete")
# Mahalanobis
clusterAv.M<-hclust(DM,method= "average")
clusterW.M<-hclust(DM,method= "ward.D2")
clusterS.M<-hclust(DM,method= "single")
clusterC.M<-hclust(DM,method= "complete")
```

Distància Euclidea (ho faig per comparar amb la Mahalanobis)
```{r, fig.height = 7, fig.width = 6, fig.align = "center", echo=FALSE}
par(mfrow=c(2,2))
plot(clusterAv.E,cex=0.6)
plot(clusterW.E,cex=0.6)
plot(clusterS.E,cex=0.6)
plot(clusterC.E,cex=0.6)
```

Només veiem una classificació homogènia en el dendograma pel mètode de Ward, utilitzant la distància Euclídea. k=2

Distància Mahalanobis:
```{r, fig.height = 7, fig.width = 6, fig.align = "center", echo=FALSE}
par(mfrow=c(2,2))
plot(clusterAv.M,cex=0.6)
plot(clusterW.M,cex=0.6)
plot(clusterS.M,cex=0.6)
plot(clusterC.M,cex=0.6)
```

Utilitzant la distància de Mahalanobis, veiem que el mètode de Ward ens dona les opcions de k=2,3,4. Tot i que sembla que un grup tingui molt pocs individus.

### Correlació cofenètica
```{r}
W.coph.E<-cophenetic(clusterW.E)
W.coph.M<-cophenetic(clusterW.M)

par(mfrow=c(1,2))
plot(W.coph.E,D)
text(1,12,round(cor(W.coph.E,D)^2,2))
plot(W.coph.M,D)
text(1,12,round(cor(W.coph.M,DM)^2,2))
```
```{r}
groups.2_E<-cutree(clusterW.E,2)  # 2 groups
table(groups.2_E)
plot(clusterW.E,hang=-1,main="Ward method, Euclidean distance",cex=0.65)
rect.hclust(clusterW.E,k=2,border="blue")

groups.2_M<-cutree(clusterW.M,2)  # 2 groups
table(groups.2_M)
plot(clusterW.M,hang=-1,main="Ward method, Mahalanobis distance",cex=0.65)
rect.hclust(clusterW.M,k=2,border="blue")

```

Amb la distància de Mahalanobis, la clusterització jeràrquica no dóna bons resultats.

## Clustering no jeràrquic

Per al clustering jeràrquic utilitzarem l'algoritme PAM , ja que la distància escollida no és Euclidea.

### Número de clusters:

Analitzarem el número de clusters amb els mètodes de Pseudo-F, Silueta i TESS. Analitzarem des de 2 fins a 10 clusters.

Pseudo-F:
```{r}
library(clusterSim)
cat("Pseudo F","\n")
PseudoF<-data.frame()
for (centers in c(2:10)){
  km.mah<-pam(DM,centers,diss=T)  # Partitioning Around Medoids
  PseudoF.mah<-index.G1(x=X, cl=km.mah$cluster, d=DM, centrotypes = "medoids")
  PseudoF[centers-1,1]<-centers
  PseudoF[centers-1,2]<-PseudoF.mah
}
PseudoF

```

Hi ha un màxim a k=2, k=5.

Silueta:
```{r}
library(factoextra)
sil<-fviz_nbclust(X, pam, diss=DM, method = "sil")   # by default function
sil
sil$data
```

El nombre òptim és k = 2

TESS
```{r}
TESS<-fviz_nbclust(X, FUNcluster= pam, diss=NULL, k.max=10, method = "wss")
#"wss" for total within sum of square 
TESS # graphical output
TESS$data #TESS values with respect number of clusters

```

```{r}
source("func.R")
TESS2<-my_fviz_nbclust(X, FUNcluster= pam, diss=NULL, k.max=10, method = "wss") 
TESS2
TESS2$data
```


```{r}
cat('de 1 a 2','\n')
(TESS$data[1,2]-TESS2$data[2,2])*100/TESS2$data[1,2]
cat('de 2 a 3','\n')
(TESS2$data[2,2]-TESS2$data[3,2])*100/TESS2$data[2,2]
cat('de 3 a 4','\n')
(TESS2$data[3,2]-TESS2$data[4,2])*100/TESS2$data[3,2]
cat('de 4 a 5','\n')
(TESS2$data[4,2]-TESS2$data[5,2])*100/TESS2$data[4,2]
cat('de 5 a 6','\n')
(TESS2$data[5,2]-TESS2$data[6,2])*100/TESS2$data[5,2]
cat('de 6 a 7','\n')
(TESS2$data[6,2]-TESS2$data[7,2])*100/TESS2$data[6,2]
cat('de 7 a 8','\n')
(TESS2$data[7,2]-TESS2$data[8,2])*100/TESS2$data[7,2]
```

Pel criteri del TESS, el millor nombre de clusters és k=2.

Tenim que k=2 és el valor més comú entre els mètodes, execturem el PAM amb k = 2

### Execució de PAM amb k=2

REALMENT TÉ SENTIT FER NOMÉS 2 GRUPS? DIVIDIREM LES ACTIVITATS EN 2 GRUPS? POTSER HAURÍEM DE MIRAR SI AMB UNA ALTRA K TENIM MILLORS RESULTATS

```{r}
k<-2
pam.2 <- pam(DM,k,diss=T)
```

Quants individus hi ha en cada grup?

```{r}
table(pam.2$clustering)
```

Separació homogènia

Representació dels 2 grups
```{r}
X[pam.2$medoids,]

# Mostrar en un gràfic com queden els grups (2 barplots, 1 amb colors complementaris...)
```

### Execució de PAM amb k=5

```{r}
k<-5
pam.5 <- pam(DM,k,diss=T)
table(pam.5$clustering)
X[pam.5$medoids,]
```

Explicació de cada grup

## Representació en 2 eixos

Com que hem utilitzat el mètode de PAM, per tal de fer la representació gràfica, no podem utilitzar un PCA, sinó un MDS.

(NO SURT BÉ)

### MDS
```{r}
mds.pam <- cmdscale(DM, k= 5, eig=TRUE)
round((mds.pam$eig)/sum(mds.pam$eig),3)[1:5]

mds.pam$GOF

plot(mds.pam$points[,1:2], col = pam.5$clustering, pch = pam.5$clustering, cex = 1, main="PAM", xlab="Axis 1", ylab="Axis 2")
```

```{r}
for (i in unique(pam.5$clustering)){
  cat("Cluster", i, "\n")
  print(apply(data[pam.5$clustering==i,],2,summary))
}

```

Explicació dels eixos de coordenades.

```{r}
cat("Eix 1", "\n")
(Axis1<-round(cor(mds.pam$points[,1], data),3))
cat("Eix 2", "\n")
(Axis2<-round(cor(mds.pam$points[,2], data),3))
```


### Canonical discriminant analysis
(MIRAR SI FUNCIONA O NO)


```{r}
W<-matrix(rep(0,ncol(X)*ncol(X)),ncol(X),ncol(X)) # matriu de 10x10 que son les variables que tenim disponibles al conjunt de dades.

classes<-unique(pam.5$clustering)  # k=6


for(i in 1:length(classes)){
  sel<-which(pam.5$clustering==classes[i])
  n<-length(sel)
  S<-cov(X[sel,])
  W<-W+S*(n-1)      # necesitamos la S no corregida
}

Spooled<-W/(nrow(X)-k)
Spooled
```
Obtenim una matriu de variances-covariances de la variabilitat intra grups

#### Between variability (Step 2)

```{r}
M<-apply(X,2,mean)
M # general mean vector
B<-matrix(rep(0,ncol(X)*ncol(X)),ncol(X),ncol(X))
for(i in 1:length(classes)){
  sel<-which(pam.5$clustering==classes[i])
  n<-length(sel)
  subX<-X[sel,]
  m<-apply(subX,2,mean)
  B<-B+n*(m-M)%*%t(m-M)
}
B
```
Matriu var-cov entre grups

#### Find new coordinates (Step 3)
```{r}
A<-solve(Spooled)%*%B
# A<-(Spooled)%*%B
vecs<-eigen(A)$vectors
Y<-as.matrix(X)%*%vecs
```


#### Data representation (Step 4)

```{r}
df<-data.frame(Y[,1],Y[,2],as.factor(pam.5$cluster))
colnames(df)<-c("Dim1", "Dim2", "cluster")
ggplot(df, aes(Dim1, Dim2, color = cluster))+
  geom_point() #+
  # stat_ellipse(type = "euclid", level=0.9)+coord_fixed()
```

Alternatively, convex hull representation (In the plane it is the smallest convex polygon that contains the points).


```{r}
find_hull <- function(df) df[chull(df$Dim1,df$Dim2),]
hulls <- ddply(df,"cluster", find_hull)
ggplot(df, aes(x=Dim1, y=Dim2, color=cluster, 
                fill=cluster)) +
geom_polygon(data=hulls,alpha=0.5) +
geom_point(size=1) 
```

#### Understanding data representation (Step 5)
```{r}
## Interpretacion de los ejes de representación
round(cor(Y[,1],as.matrix(X)),2)
round(cor(Y[,2],as.matrix(X)),2)
```








## Test multivariant

### Normalitat
Check whether the multivariate skewness and kurtosis are consistent with a multivariate normal distribution.
Bajo Ho, el estadístico skewness es khi-2 y el estadistico de la kurtosis es N(0,1).

```{r}
library(MVN)
for(i in 1:k){
  data.subset<-X[pam.5$clustering==i,]
  cat('cluster', i, " Determinant = ", round(det(cov(data.subset)), 3), '\n')
  res<-mvn(data.subset,mvnTest="mardia",multivariatePlot="qq") #
  print(res$multivariateNormality)
  print(res$univariateNormality)
}
```

En el test de normalidad, NO se puede aceptar la hipótesis nula, que asume que los todos datos son normales.

Hacemos test de normalidad univariante y vemos que no son normales. Si ellas solas no son normales, el conjunto no será norma

```{r}
library(biotools)  # Homogenety variances

# Box's M-test for homogeneity of covariance matrices
boxM(X,pam.5$clustering)

```
Var-Cov matrix non constant


#### Pairwise comparison
Si no són normals, no es pot utilitzar l'estadístic T-Hotellin. Amb la funció hotelling.test li posem els grups que volem comparar (g1 i g2) i li fem el test chi-quadrat

```{r}
library(Hotelling)  # Hotelling

g1<-X[pam.5$clustering==1,]
g2<-X[pam.5$clustering==2,]

ht12<-hotelling.test(g1, g2, test="chi") # 
ht12
```